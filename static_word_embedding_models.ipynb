{"cells":[{"cell_type":"markdown","metadata":{"id":"IG4XMSVBH2Ny"},"source":["# Introduction\n","\n","**Word Embedding History**\n","\n","Word embeddings are a type of representation for natural language processing tasks in which words are represented by numeric vectors. These vectors capture the meaning of the words and the relationships between them.\n","\n","The idea of representing words as numeric vectors dates back to at least the 1960s, with the development of word2vec in the early 2010s being a significant milestone in the history of word embeddings.\n","\n","Word2vec is a machine learning model that is trained to predict a target word given a context word, or vice versa. During training, the model learns the relationships between words and encodes them as numeric vectors, or \"word embeddings\". These embeddings can then be used as input to other natural language processing tasks, such as text classification or machine translation.\n","\n","Since the development of word2vec, there have been many other approaches to generating word embeddings, including fastText, GloVe, and BERT. These approaches have improved upon the original word2vec model and have allowed for the creation of large, high-quality word embedding models that are widely used in natural language processing tasks.\n","\n","\n","**Table of Contents**\n","\n","* INTRODUCTION\n","    * Word Embedding History\n","    * Table of Contents\n","\n","1. Word2Vec\n","    1. Training Your Own word2vec Model\n","    2. Using a Pretrained \"word2vec\" Model\n","    3. Details about Word2vec Implementation\n","    4. Word2vec for Recommendation\n","\n","2. Other Approaches\n","    1. GloVe\n","    2. fastText\n","        1.  Subword Tokenization\n","\n","* References\n","\n","* Acknowledgment\n"]},{"cell_type":"markdown","metadata":{"id":"Dw6Sr3ouItu8"},"source":["# 1. Word2Vec\n","\n","**Original Paper**: Mikolov at al. (2013a)\n","\n","*Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that*\n","$$\\overrightarrow{\\text{king}} \\ - \\ \\overrightarrow{\\text{man}} \\ + \\ \\overrightarrow{\\text{woman}} \\ \\approx \\ \\overrightarrow{\\text{queen}}$$\n"]},{"cell_type":"markdown","metadata":{"id":"-wxZKo95i6YC"},"source":["**Self-supervised: The Fake Task**\n","\n","<p align=\"center\">\n","  <img src=\"https://jalammar.github.io/images/word2vec/continuous-bag-of-words-example.png\" alt=\"\" width=\"600\">\n","</p>\n","\n","<center>Image source: Alammar, Jay (2019) </center>\n","\n","The word in the green slot would be the input word, each pink box would be a possible output.\n","\n","<p align=\"center\">\n","  <img src=\"https://jalammar.github.io/images/word2vec/continuous-bag-of-words-dataset.png\" alt=\"\" width=\"600\">\n","</p>\n","\n","<center>Image source: Alammar, Jay (2019) </center>\n","\n","Words in the green slots would be the input, the pink box would be the output word.\n","\n","<p align=\"center\">\n","  <img src=\"https://jalammar.github.io/images/word2vec/skipgram-sliding-window-samples.png\" alt=\"\" width=\"600\">\n","</p>\n","\n","<center>Image source: Alammar, Jay (2019) </center>"]},{"cell_type":"markdown","metadata":{"id":"Ld-c4GeygKDb"},"source":["**Model Architectures**\n","\n","1. Continuous Bag-of-Words Model (CBOW): The CBOW architecture predicts the current word based on the context.\n","2. Continuous Skip-gram Model: Skip-gram predicts surrounding words given the current word.\n","\n","<p align=\"center\">\n","  <img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2018/01/18/sagemaker-word2vec-1.gif\" alt=\"\" width=\"500\">\n","</p>\n","\n","<center>Image source: Mikolov at al. (2013a) </center>\n","\n","It's worth mentioning that, beyond the architecture, the way to build the training dataset also changes. \n","\n","For CBOW, each word generates a single training datapoint, while for Skip-gram, each word generates 2N datapoints, where N is the number of surrounding words for each side to consider (Note that 2N+1 is equal to the window size), exactly as shown in the previous example."]},{"cell_type":"markdown","metadata":{"id":"8owiQSFSgKeM"},"source":["**Under the Hood: Continuous Skip-gram Model**\n","\n","<p align=\"center\">\n","  <img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" alt=\"\" width=\"600\">\n","</p>\n","\n","<center>Image source: McCormick, Chris (2016) </center>"]},{"cell_type":"markdown","metadata":{"id":"G-gpcRwYi660"},"source":["**From the neural network to the word embeddings**\n","\n","<p align=\"center\">\n","  <img src=\"https://lilianweng.github.io/posts/2017-10-15-word-embedding/word2vec-skip-gram.png\" alt=\"\" width=\"600\">\n","</p>\n","\n","<center>Image source: Weng, Lilian (2017) </center>"]},{"cell_type":"markdown","metadata":{"id":"P3dEueI1Iz0X"},"source":["## 1.1. Training Your Own word2vec Model\n","\n","Code source: Patel, Dhaval [codebasics] (2021a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVXIlpG2EpKM"},"outputs":[],"source":["!pip install gensim==3.6.0 --quiet\n","!pip install nltk==3.7 --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4241,"status":"ok","timestamp":1674754381530,"user":{"displayName":"Luis Antonio Rodrigues","userId":"07263425132269744262"},"user_tz":0},"id":"o5cSqDjNI3l4","outputId":"e0ed34bf-21e6-40f4-8b5a-e772a330725e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from gensim.models import Word2Vec\n","import gensim.downloader\n","import nltk\n","from nltk.corpus import brown\n","nltk.download(\"brown\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1674754381531,"user":{"displayName":"Luis Antonio Rodrigues","userId":"07263425132269744262"},"user_tz":0},"id":"hemb8WbRI6rV","outputId":"d9016cac-9271-4fc2-e2fd-c4720af58872"},"outputs":[{"data":{"text/plain":["[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Define a corpus to train as Brown Corpus\n","# https://en.wikipedia.org/wiki/Brown_Corpus\n","corpus = brown.sents()\n","corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47385,"status":"ok","timestamp":1674754438507,"user":{"displayName":"Luis Antonio Rodrigues","userId":"07263425132269744262"},"user_tz":0},"id":"MjEdw8reI6i2","outputId":"1b9baa88-e62f-4d63-8a8b-e72ba3c5301a"},"outputs":[{"data":{"text/plain":["(4270788, 5805960)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize the model\n","model = Word2Vec(size=100, window=5, min_count=1, workers=4)\n","\n","# Build Vocabulary\n","model.build_vocab(corpus, progress_per=1000)\n","\n","# Train the Word2Vec Model\n","model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1674754439969,"user":{"displayName":"Luis Antonio Rodrigues","userId":"07263425132269744262"},"user_tz":0},"id":"cuUE8RYbJCh-","outputId":"907e2d86-819a-435c-8542-721183bfffbb"},"outputs":[{"data":{"text/plain":["array([-1.3682545 ,  0.8700504 ,  0.84637284, -0.28152472, -0.36479753,\n","        0.62864673, -0.39947757, -0.19395827, -0.04070418, -0.12854913,\n","       -0.08353873,  0.37000635,  0.3129534 , -0.4955536 ,  0.16299991,\n","        0.20593488,  0.15088503,  0.51814723,  0.27549544, -0.02539549,\n","        0.3392709 ,  0.0812735 , -0.8843315 ,  0.8175514 , -0.36355928,\n","        0.47324288, -0.3648843 ,  0.04472586, -0.5131664 , -0.04934174,\n","        0.4114774 ,  0.8081736 , -0.4333348 ,  0.33533475,  0.12678465,\n","        0.26695752,  0.42117417,  0.37199467,  0.44636178, -0.4233144 ,\n","       -0.2165184 ,  0.73775876, -0.7551475 , -0.82751644, -0.06093928,\n","        0.12336696, -0.30301565, -0.2139607 , -0.5781787 , -0.4914802 ,\n","        0.75109476,  0.36604956,  0.35860997, -0.595864  , -0.15172918,\n","        0.12596309,  0.57722074, -0.29941458,  0.02710133, -0.6827911 ,\n","        0.28633878,  0.5538602 , -0.34357   , -0.80099106, -0.5399351 ,\n","        0.43680775, -0.6101016 , -1.0071228 , -0.38077193,  0.38594684,\n","        0.14376682, -0.26320076, -0.21925582,  0.10775812,  0.24350922,\n","       -0.28928337,  0.6041741 , -0.00423847,  1.1566942 , -0.35072038,\n","       -0.04763994,  0.6943211 ,  0.30813915, -0.18535869,  0.30503547,\n","        0.11856415,  0.28586268, -0.13551886,  0.27859727,  0.46541333,\n","       -0.1310272 ,  0.2811517 ,  0.40170926,  0.22906926,  0.05143757,\n","       -1.0188231 , -0.01432456,  0.5582081 ,  0.00454884,  0.12329286],\n","      dtype=float32)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Get the embedding for a word\n","vector = model.wv['Government']\n","vector"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1674754537344,"user":{"displayName":"Luis Antonio Rodrigues","userId":"07263425132269744262"},"user_tz":0},"id":"A2Ptb4f3JUdc","outputId":"0725c4bb-7518-4e91-cd46-51539df30e11"},"outputs":[{"name":"stdout","output_type":"stream","text":["'Government'\t'rich'\t0.89\n","'Government'\t'communism'\t0.82\n","'Government'\t'math'\t0.56\n","'Government'\t'people'\t0.49\n"]}],"source":["# Compute the similarity between two words\n","pairs = [\n","    ('Government', 'rich'),\n","    ('Government', 'communism'),\n","    ('Government', 'math'),\n","    ('Government', 'people'),\n","]\n","for w1, w2 in pairs:\n","    print('%r\\t%r\\t%.2f' % (w1, w2, model.wv.similarity(w1, w2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":212,"status":"ok","timestamp":1674754515556,"user":{"displayName":"Luis Antonio Rodrigues","userId":"07263425132269744262"},"user_tz":0},"id":"37ut3PC5Jdsk","outputId":"d97902ce-d671-4f8c-d27c-0e0dfbbb2fa8"},"outputs":[{"data":{"text/plain":["[('Federal', 0.9730518460273743),\n"," ('board', 0.9717510342597961),\n"," ('press', 0.9701008796691895),\n"," ('strengthening', 0.9700838327407837),\n"," ('reaction', 0.9695568084716797),\n"," ('operation', 0.9675626754760742),\n"," ('Union', 0.9674526453018188),\n"," ('Constitution', 0.9671463370323181),\n"," ('Soviet', 0.9661285877227783),\n"," ('link', 0.9656205177307129)]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Get the most similar words to a given word\n","model.wv.most_similar(\"Government\")"]},{"cell_type":"markdown","metadata":{"id":"dzDZQfr3JmYH"},"source":["## 1.2. Using a Pretrained \"word2vec\" Model\n","\n","Code source: Řehůřek, Radim (2022)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1674754562596,"user":{"displayName":"Luis Antonio Rodrigues","userId":"07263425132269744262"},"user_tz":0},"id":"0MgHQiTTJnen","outputId":"7c6bbbe7-dfd6-4349-c12b-e7542fe26d32"},"outputs":[{"data":{"text/plain":["['fasttext-wiki-news-subwords-300',\n"," 'conceptnet-numberbatch-17-06-300',\n"," 'word2vec-ruscorpora-300',\n"," 'word2vec-google-news-300',\n"," 'glove-wiki-gigaword-50',\n"," 'glove-wiki-gigaword-100',\n"," 'glove-wiki-gigaword-200',\n"," 'glove-wiki-gigaword-300',\n"," 'glove-twitter-25',\n"," 'glove-twitter-50',\n"," 'glove-twitter-100',\n"," 'glove-twitter-200',\n"," '__testing_word2vec-matrix-synopsis']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Show all available models in gensim-data\n","list(gensim.downloader.info()['models'].keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68302,"status":"ok","timestamp":1674754640424,"user":{"displayName":"Luis Antonio Rodrigues","userId":"07263425132269744262"},"user_tz":0},"id":"r5h2AWj4JqSA","outputId":"8e73ddf0-6d94-425e-ceb7-346c389645c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["[==================================================] 100.0% 104.8/104.8MB downloaded\n"]}],"source":["# Download the \"glove-twitter-25\" embeddings\n","glove_vectors = gensim.downloader.load('glove-twitter-25')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":724,"status":"ok","timestamp":1674754876890,"user":{"displayName":"Luis Antonio Rodrigues","userId":"07263425132269744262"},"user_tz":0},"id":"7ft3L15RJsxT","outputId":"4566c343-030f-4a4b-fd14-656e22865bc5"},"outputs":[{"data":{"text/plain":["[('facebook', 0.9480051398277283),\n"," ('tweet', 0.9403422474861145),\n"," ('fb', 0.9342358708381653),\n"," ('instagram', 0.9104823470115662),\n"," ('chat', 0.8964964747428894),\n"," ('hashtag', 0.8885936141014099),\n"," ('tweets', 0.8878157734870911),\n"," ('tl', 0.8778461813926697),\n"," ('link', 0.877821147441864),\n"," ('internet', 0.8753897547721863)]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Use the downloaded vectors as usual, ex:\n","glove_vectors.most_similar('twitter')"]},{"cell_type":"markdown","metadata":{},"source":["## 1.3. Details about Word2vec Implementation\n","\n","It is hard to train a word2vec model formulated as presented, once it is a huge neural network.\n","\n","So, in a second paper, Mikolov at al. (2013b), the author presented several extensions that improve both the quality of the vectors and the training speed, mainly:\n","- Subsampling frequent words during training, which results in a significant speedup (around 2x - 10x), and\n","improves accuracy of the representations of less frequent words.\n","- Developing a simple alternative to the hierarchical softmax called **Negative Sampling**, which is a simplified variant of Noise Contrastive Estimation (NCE) for training the Skip-gram model that results in faster training and better vector representations for frequent words, compared to more complex hierarchical softmax that was used in the prior work.\n","\n","Suggested resources:\n","* Alammar, Jay (2019)\n","* McCormick, Chris (2017)\n"]},{"cell_type":"markdown","metadata":{"id":"hDm2h6lqKvJE"},"source":["## 1.4. Word2vec for Recommendation\n","\n","<p align=\"center\">\n","  <img src=\"https://cdn-images-1.medium.com/max/1400/1*xbNM_CnEIWQtGbsLmZtE-A.gif\" alt=\"\" width=\"600\">\n","</p>\n","\n","<center>Image source: Karam, Ramzi (2017) </center>\n","\n","Suggested resources:\n","* McCormick, Chris (2018)\n","* Karam, Ramzi (2017)"]},{"cell_type":"markdown","metadata":{"id":"q6uzguSxLuxa"},"source":["# 2. Other Approaches\n","\n","## 2.1.  GloVe\n","\n","**Original Paper**: Pennington at al. (2014)\n","\n","GloVe (Global Vectors) is a word embedding model that represents words as numeric vectors in a high-dimensional space. These vectors capture the meaning of the words and the relationships between them, and they can be used as input to a variety of natural language processing tasks.\n","\n","GloVe was developed by Stanford University researchers in 2014 as an extension of the word2vec model, which was introduced in the early 2010s. Like word2vec, GloVe represents words as vectors in a high-dimensional space, but it uses a different training objective and a different algorithm to learn the word embeddings.\n","\n","One of the main advantages of GloVe is that it is able to learn meaningful word embeddings from very large corpora of text, even when the corpora are very sparse (i.e., when the number of words in the corpus is much larger than the number of unique words). This makes GloVe particularly well-suited for tasks that require the use of very large text corpora, such as language translation and language modeling.\n","\n","GloVe has been widely used in natural language processing tasks and has achieved state-of-the-art performance on many benchmarks. It is also available in a variety of open-source software libraries, making it easy to use in a variety of applications. However, like all machine learning models, it has some limitations and disadvantages:\n","\n","1. GloVe requires a large amount of training data to learn effective word embeddings. This can be a disadvantage if you do not have access to a large enough corpus of text to train the model.\n","\n","2. GloVe is a computationally intensive model to train. It requires a large amount of computation to learn the word embeddings, and this can be a disadvantage if you do not have access to sufficient computational resources.\n","\n","3. **GloVe is a \"static\" word embedding model, which means that the word embeddings are fixed after the model is trained and do not change based on the context in which the words appear.** This can be a disadvantage in tasks that require context-sensitive word embeddings, such as language modeling or machine translation.\n","\n","4. *GloVe is a \"linear\" word embedding model, which means that it represents words as linear combinations of a small number of basis vectors.* This can be a disadvantage in tasks that require more complex, non-linear representations of words, such as image recognition or speech recognition.\n","\n","## 2.2.  fastText\n","\n","**Original Paper**: Bojanowski at al. (2016)\n","\n","\"As the name suggests, fastText is a fast-to-train word representation based on the Word2Vec skip-gram model, that can be trained on more than one billion words in less than ten minutes using a standard multicore CPU.\n","\n","fastText can address limitations 3 [Word2Vec cannot understand out-of-vocabulary (OOV) words, i.e. words not present in training data. You could assign a UNK token which is used for all OOV words or you could use other models that are robust to OOV words.] and 4 [By assigning a distinct vector to each word, Word2Vec ignores the morphology of words. For example, eat, eats, and eaten are considered independently different words by Word2Vec, but they come from the same root: eat, which might contain useful information.] (...) \n","\n","The model learns word representations while also taking into account morphology, which is captured by considering subword units (character n-grams).\" (Uzila, 2012)\n","\n","### 2.2.1  Subword tokenization\n","\n","Two most famous techniques:\n","* Byte Pair Encoding (used in GPT, for example)\n","* WordPiece (used in BERT, for example)\n","\n","Suggested resources:\n","* Krohn, Jon (2022)\n","* Huggingface (2021a)\n","* Huggingface (2021b)\n"]},{"cell_type":"markdown","metadata":{"id":"L9_8zyG_Ih_2"},"source":["# References\n","\n","\n","---\n","## Papers\n","\n","* Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey. 2013a. Efficient Estimation of Word Representations in Vector Space, arXiv, <https://arxiv.org/abs/1301.3781.pdf>\n","\n","* Mikolov, Tomas and, Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey. 2013b. Distributed Representations of Words and Phrases and their Compositionality, arXiv, <https://arxiv.org/abs/1310.4546.pdf>\n","\n","* Pennington, Jeffrey and Socher, Richard and Manning, Christopher. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar. Association for Computational Linguistics. <https://aclanthology.org/D14-1162/>\n","\n","\n","\n","* Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas. 2016. Enriching Word Vectors with Subword Information, arXiv, <https://arxiv.org/abs/1607.04606>\n","\n","---\n","## Articles in Magazines/Sites\n","\n","* Karam, Ramzi (2017). Using Word2vec for Music Recommendations. Towards Data Science, <https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484>\n","\n","\n","* Uzila, Albers (2012). GloVe and fastText Clearly Explained: Extracting Features from Text Data. Level Up Coding, <https://levelup.gitconnected.com/glove-and-fasttext-clearly-explained-extracting-features-from-text-data-1d227ab017b2>\n","\n","---\n","## Web Pages\n","\n","* Alammar, Jay (2019). The Illustrated Word2vec, accessed January 2023, <https://jalammar.github.io/illustrated-word2vec/>\n","\n","* McCormick, Chris (2016). Word2Vec Tutorial - The Skip-Gram Model, accessed December 2022, <http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/>\n","\n","* McCormick, Chris (2017). Word2Vec Tutorial Part 2 - Negative Sampling, accessed January 2023, <http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/>\n","\n","\n","* McCormick, Chris (2018). Applying word2vec to Recommenders and Advertising, accessed January 2023, <http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/>\n","\n","\n","* Řehůřek, Radim (2022). Word2vec embeddings, accessed December 2022, <https://radimrehurek.com/gensim/models/word2vec.html>\n","\n","\n","* Weng, Lilian (2017). Learning Word Embedding, accessed January 2023, <https://lilianweng.github.io/posts/2017-10-15-word-embedding/>\n","\n","\n","---\n","## Videos and Podcasts\n","\n","* Patel, Dhaval [codebasics] (2021a). Word2Vec Part 2 | Implement word2vec in gensim | | Deep Learning Tutorial 42 with Python. YouTube, accessed December 2022, <https://www.youtube.com/watch?v=Q2NtCcqmIww&t=0s>\n","\n","* Krohn, Jon (2022). SDS 626: Subword Tokenization with Byte-Pair Encoding. Super Data Science Podcast, accessed December 2022, <https://www.superdatascience.com/podcast/subword-tokenization-with-byte-pair-encoding>\n","\n","* Huggingface (2021a). Byte Pair Encoding Tokenization. YouTube, accessed December 2022, <https://www.youtube.com/watch?v=HEikzVL-lZU&t=1s> (and <https://huggingface.co/course/chapter6/5>)\n","\n","* Huggingface (2021b). WordPiece Tokenization. YouTube, accessed December 2022, <https://www.youtube.com/watch?v=qpv6ms_t_1A> (and <https://huggingface.co/course/chapter6/6>)"]},{"cell_type":"markdown","metadata":{"id":"_NoSrODSIOMB"},"source":["# Acknowledgment\n","\n","Notebook texts were powered by OpenAI's ChatGPT."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN6HIhHL+bEr3qwiEcoF8Qp","provenance":[]},"kernelspec":{"display_name":"Python 3.10.4 ('data-driven-innovation-env')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.4"},"vscode":{"interpreter":{"hash":"713703aeb090dba4719e38c6e79ac9d70ef12abfaf376155ceb2bbce6c79c673"}}},"nbformat":4,"nbformat_minor":0}
