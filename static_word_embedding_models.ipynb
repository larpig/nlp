{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzCq/c9G/OEMNdvcYHGq07",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larpig/nlp/blob/main/static_word_embedding_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "**Word Embedding History**\n",
        "\n",
        "Word embeddings are a type of representation for natural language processing tasks in which words are represented by numeric vectors. These vectors capture the meaning of the words and the relationships between them.\n",
        "\n",
        "The idea of representing words as numeric vectors dates back to at least the 1960s, with the development of word2vec in the early 2010s being a significant milestone in the history of word embeddings.\n",
        "\n",
        "Word2vec is a machine learning model that is trained to predict a target word given a context word, or vice versa. During training, the model learns the relationships between words and encodes them as numeric vectors, or \"word embeddings\". These embeddings can then be used as input to other natural language processing tasks, such as text classification or machine translation.\n",
        "\n",
        "Since the development of word2vec, there have been many other approaches to generating word embeddings, including fastText, GloVe, and BERT. These approaches have improved upon the original word2vec model and have allowed for the creation of large, high-quality word embedding models that are widely used in natural language processing tasks.\n",
        "\n",
        "\n",
        "**Table of Contents**\n",
        "\n",
        "* INTRODUCTION\n",
        "    * Word Embedding History\n",
        "    * Table of Contents\n",
        "\n",
        "1. Word2Vec\n",
        "    1. Training Your Own word2vec Model\n",
        "    2. Using a Pretrained \"word2vec\" Model\n",
        "    3. Word2vec for Recommendation\n",
        "\n",
        "2. Other Approaches\n",
        "    1. GloVe\n",
        "    2. fastText\n",
        "        1.  Subword Tokenization\n",
        "\n",
        "* References\n",
        "\n",
        "* Acknowledgment\n"
      ],
      "metadata": {
        "id": "IG4XMSVBH2Ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Word2Vec\n",
        "\n",
        "**Original Paper**: Mikolov at al. (2013)\n",
        "\n",
        "*Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that*\n",
        "$$\\overrightarrow{\\text{king}} \\ - \\ \\overrightarrow{\\text{man}} \\ + \\ \\overrightarrow{\\text{woman}} \\ \\approx \\ \\overrightarrow{\\text{queen}}$$\n"
      ],
      "metadata": {
        "id": "Dw6Sr3ouItu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architectures**\n",
        "\n",
        "1. Continuous Bag-of-Words Model (CBOW)\n",
        "2. Continuous Skip-gram Model\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2018/01/18/sagemaker-word2vec-1.gif\" alt=\"\" width=\"500\">\n",
        "</p>\n",
        "\n",
        "<center>Image source: Mikolov at al. (2013) </center>\n",
        "\n",
        "The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.\n"
      ],
      "metadata": {
        "id": "Ld-c4GeygKDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self-supervised: The Fake Task**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"http://mccormickml.com/assets/word2vec/training_data.png\" alt=\"\" width=\"600\">\n",
        "</p>\n",
        "\n",
        "<center>Image source: McCormick, Chris (2016) </center>"
      ],
      "metadata": {
        "id": "-wxZKo95i6YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Under the Hood: Continuous Skip-gram Model**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" alt=\"\" width=\"600\">\n",
        "</p>\n",
        "\n",
        "<center>Image source: McCormick, Chris (2016) </center>"
      ],
      "metadata": {
        "id": "8owiQSFSgKeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the neural network to the word embeddings**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://lilianweng.github.io/posts/2017-10-15-word-embedding/word2vec-skip-gram.png\" alt=\"\" width=\"600\">\n",
        "</p>\n",
        "\n",
        "<center>Image source: Weng, Lilian (2017) </center>"
      ],
      "metadata": {
        "id": "G-gpcRwYi660"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Training Your Own word2vec Model\n",
        "\n",
        "Code source: Patel, Dhaval [codebasics] (2021a)"
      ],
      "metadata": {
        "id": "P3dEueI1Iz0X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVXIlpG2EpKM"
      },
      "outputs": [],
      "source": [
        "!pip install gensim==3.6.0 --quiet\n",
        "!pip install nltk==3.7 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download(\"brown\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5cSqDjNI3l4",
        "outputId": "e0ed34bf-21e6-40f4-8b5a-e772a330725e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a corpus to train as Brown Corpus\n",
        "# https://en.wikipedia.org/wiki/Brown_Corpus\n",
        "corpus = brown.sents()\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hemb8WbRI6rV",
        "outputId": "d9016cac-9271-4fc2-e2fd-c4720af58872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = Word2Vec(size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Build Vocabulary\n",
        "model.build_vocab(corpus, progress_per=1000)\n",
        "\n",
        "# Train the Word2Vec Model\n",
        "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjEdw8reI6i2",
        "outputId": "1b9baa88-e62f-4d63-8a8b-e72ba3c5301a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4270788, 5805960)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embedding for a word\n",
        "vector = model.wv['Government']\n",
        "vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuUE8RYbJCh-",
        "outputId": "907e2d86-819a-435c-8542-721183bfffbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.3682545 ,  0.8700504 ,  0.84637284, -0.28152472, -0.36479753,\n",
              "        0.62864673, -0.39947757, -0.19395827, -0.04070418, -0.12854913,\n",
              "       -0.08353873,  0.37000635,  0.3129534 , -0.4955536 ,  0.16299991,\n",
              "        0.20593488,  0.15088503,  0.51814723,  0.27549544, -0.02539549,\n",
              "        0.3392709 ,  0.0812735 , -0.8843315 ,  0.8175514 , -0.36355928,\n",
              "        0.47324288, -0.3648843 ,  0.04472586, -0.5131664 , -0.04934174,\n",
              "        0.4114774 ,  0.8081736 , -0.4333348 ,  0.33533475,  0.12678465,\n",
              "        0.26695752,  0.42117417,  0.37199467,  0.44636178, -0.4233144 ,\n",
              "       -0.2165184 ,  0.73775876, -0.7551475 , -0.82751644, -0.06093928,\n",
              "        0.12336696, -0.30301565, -0.2139607 , -0.5781787 , -0.4914802 ,\n",
              "        0.75109476,  0.36604956,  0.35860997, -0.595864  , -0.15172918,\n",
              "        0.12596309,  0.57722074, -0.29941458,  0.02710133, -0.6827911 ,\n",
              "        0.28633878,  0.5538602 , -0.34357   , -0.80099106, -0.5399351 ,\n",
              "        0.43680775, -0.6101016 , -1.0071228 , -0.38077193,  0.38594684,\n",
              "        0.14376682, -0.26320076, -0.21925582,  0.10775812,  0.24350922,\n",
              "       -0.28928337,  0.6041741 , -0.00423847,  1.1566942 , -0.35072038,\n",
              "       -0.04763994,  0.6943211 ,  0.30813915, -0.18535869,  0.30503547,\n",
              "        0.11856415,  0.28586268, -0.13551886,  0.27859727,  0.46541333,\n",
              "       -0.1310272 ,  0.2811517 ,  0.40170926,  0.22906926,  0.05143757,\n",
              "       -1.0188231 , -0.01432456,  0.5582081 ,  0.00454884,  0.12329286],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the similarity between two words\n",
        "pairs = [\n",
        "    ('Government', 'rich'),\n",
        "    ('Government', 'communism'),\n",
        "    ('Government', 'math'),\n",
        "    ('Government', 'people'),\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, model.wv.similarity(w1, w2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2Ptb4f3JUdc",
        "outputId": "0725c4bb-7518-4e91-cd46-51539df30e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Government'\t'rich'\t0.89\n",
            "'Government'\t'communism'\t0.82\n",
            "'Government'\t'math'\t0.56\n",
            "'Government'\t'people'\t0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the most similar words to a given word\n",
        "model.wv.most_similar(\"Government\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ut3PC5Jdsk",
        "outputId": "d97902ce-d671-4f8c-d27c-0e0dfbbb2fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Federal', 0.9730518460273743),\n",
              " ('board', 0.9717510342597961),\n",
              " ('press', 0.9701008796691895),\n",
              " ('strengthening', 0.9700838327407837),\n",
              " ('reaction', 0.9695568084716797),\n",
              " ('operation', 0.9675626754760742),\n",
              " ('Union', 0.9674526453018188),\n",
              " ('Constitution', 0.9671463370323181),\n",
              " ('Soviet', 0.9661285877227783),\n",
              " ('link', 0.9656205177307129)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Using a Pretrained \"word2vec\" Model\n",
        "\n",
        "Code source: Řehůřek, Radim (2022)"
      ],
      "metadata": {
        "id": "dzDZQfr3JmYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show all available models in gensim-data\n",
        "list(gensim.downloader.info()['models'].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MgHQiTTJnen",
        "outputId": "7c6bbbe7-dfd6-4349-c12b-e7542fe26d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fasttext-wiki-news-subwords-300',\n",
              " 'conceptnet-numberbatch-17-06-300',\n",
              " 'word2vec-ruscorpora-300',\n",
              " 'word2vec-google-news-300',\n",
              " 'glove-wiki-gigaword-50',\n",
              " 'glove-wiki-gigaword-100',\n",
              " 'glove-wiki-gigaword-200',\n",
              " 'glove-wiki-gigaword-300',\n",
              " 'glove-twitter-25',\n",
              " 'glove-twitter-50',\n",
              " 'glove-twitter-100',\n",
              " 'glove-twitter-200',\n",
              " '__testing_word2vec-matrix-synopsis']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the \"glove-twitter-25\" embeddings\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5h2AWj4JqSA",
        "outputId": "8e73ddf0-6d94-425e-ceb7-346c389645c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the downloaded vectors as usual, ex:\n",
        "glove_vectors.most_similar('twitter')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ft3L15RJsxT",
        "outputId": "4566c343-030f-4a4b-fd14-656e22865bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('facebook', 0.9480051398277283),\n",
              " ('tweet', 0.9403422474861145),\n",
              " ('fb', 0.9342358708381653),\n",
              " ('instagram', 0.9104823470115662),\n",
              " ('chat', 0.8964964747428894),\n",
              " ('hashtag', 0.8885936141014099),\n",
              " ('tweets', 0.8878157734870911),\n",
              " ('tl', 0.8778461813926697),\n",
              " ('link', 0.877821147441864),\n",
              " ('internet', 0.8753897547721863)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Word2vec for Recommendation\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://cdn-images-1.medium.com/max/1400/1*xbNM_CnEIWQtGbsLmZtE-A.gif\" alt=\"\" width=\"600\">\n",
        "</p>\n",
        "\n",
        "<center>Image source: Karam, Ramzi (2017) </center>\n",
        "\n",
        "Suggested resources:\n",
        "* McCormick, Chris (2018)\n",
        "* Karam, Ramzi (2017)"
      ],
      "metadata": {
        "id": "hDm2h6lqKvJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Approaches\n",
        "\n",
        "## 2.1.  GloVe\n",
        "\n",
        "**Original Paper**: Pennington at al. (2014)\n",
        "\n",
        "GloVe (Global Vectors) is a word embedding model that represents words as numeric vectors in a high-dimensional space. These vectors capture the meaning of the words and the relationships between them, and they can be used as input to a variety of natural language processing tasks.\n",
        "\n",
        "GloVe was developed by Stanford University researchers in 2014 as an extension of the word2vec model, which was introduced in the early 2010s. Like word2vec, GloVe represents words as vectors in a high-dimensional space, but it uses a different training objective and a different algorithm to learn the word embeddings.\n",
        "\n",
        "One of the main advantages of GloVe is that it is able to learn meaningful word embeddings from very large corpora of text, even when the corpora are very sparse (i.e., when the number of words in the corpus is much larger than the number of unique words). This makes GloVe particularly well-suited for tasks that require the use of very large text corpora, such as language translation and language modeling.\n",
        "\n",
        "GloVe has been widely used in natural language processing tasks and has achieved state-of-the-art performance on many benchmarks. It is also available in a variety of open-source software libraries, making it easy to use in a variety of applications. However, like all machine learning models, it has some limitations and disadvantages:\n",
        "\n",
        "1. GloVe requires a large amount of training data to learn effective word embeddings. This can be a disadvantage if you do not have access to a large enough corpus of text to train the model.\n",
        "\n",
        "2. GloVe is a computationally intensive model to train. It requires a large amount of computation to learn the word embeddings, and this can be a disadvantage if you do not have access to sufficient computational resources.\n",
        "\n",
        "3. **GloVe is a \"static\" word embedding model, which means that the word embeddings are fixed after the model is trained and do not change based on the context in which the words appear.** This can be a disadvantage in tasks that require context-sensitive word embeddings, such as language modeling or machine translation.\n",
        "\n",
        "4. *GloVe is a \"linear\" word embedding model, which means that it represents words as linear combinations of a small number of basis vectors.* This can be a disadvantage in tasks that require more complex, non-linear representations of words, such as image recognition or speech recognition.\n",
        "\n",
        "## 2.2.  fastText\n",
        "\n",
        "**Original Paper**: Bojanowski at al. (2016)\n",
        "\n",
        "\"As the name suggests, fastText is a fast-to-train word representation based on the Word2Vec skip-gram model, that can be trained on more than one billion words in less than ten minutes using a standard multicore CPU.\n",
        "\n",
        "fastText can address limitations 3 [Word2Vec cannot understand out-of-vocabulary (OOV) words, i.e. words not present in training data. You could assign a UNK token which is used for all OOV words or you could use other models that are robust to OOV words.] and 4 [By assigning a distinct vector to each word, Word2Vec ignores the morphology of words. For example, eat, eats, and eaten are considered independently different words by Word2Vec, but they come from the same root: eat, which might contain useful information.] (...) \n",
        "\n",
        "The model learns word representations while also taking into account morphology, which is captured by considering subword units (character n-grams).\" (Uzila, 2012)\n",
        "\n",
        "### 2.2.1  Subword tokenization\n",
        "\n",
        "Two most famous techniques:\n",
        "* Byte Pair Encoding (used in GPT, for example)\n",
        "* WordPiece (used in BERT, for example)\n",
        "\n",
        "Suggested resources:\n",
        "* Krohn, Jon (2022)\n",
        "* Huggingface (2021a)\n",
        "* Huggingface (2021b)\n"
      ],
      "metadata": {
        "id": "q6uzguSxLuxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "\n",
        "---\n",
        "## Papers\n",
        "\n",
        "* Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey. 2013. Efficient Estimation of Word Representations in Vector Space, arXiv, <https://arxiv.org/abs/1301.3781.pdf>\n",
        "\n",
        "\n",
        "* Pennington, Jeffrey and Socher, Richard and Manning, Christopher. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar. Association for Computational Linguistics. <https://aclanthology.org/D14-1162/>\n",
        "\n",
        "\n",
        "\n",
        "* Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas. 2016. Enriching Word Vectors with Subword Information, arXiv, <https://arxiv.org/abs/1607.04606>\n",
        "\n",
        "---\n",
        "## Articles in Magazines/Sites\n",
        "\n",
        "* Karam, Ramzi (2017). Using Word2vec for Music Recommendations. Towards Data Science, <https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484>\n",
        "\n",
        "\n",
        "* Uzila, Albers (2012). GloVe and fastText Clearly Explained: Extracting Features from Text Data. Level Up Coding, <https://levelup.gitconnected.com/glove-and-fasttext-clearly-explained-extracting-features-from-text-data-1d227ab017b2>\n",
        "\n",
        "---\n",
        "## Web Pages\n",
        "\n",
        "* McCormick, Chris (2016). Word2Vec Tutorial - The Skip-Gram Model, accessed December 2022, <http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/>\n",
        "\n",
        "* McCormick, Chris (2018). Applying word2vec to Recommenders and Advertising, accessed January 2023, <http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/>\n",
        "\n",
        "\n",
        "* Řehůřek, Radim (2022). Word2vec embeddings, accessed December 2022, <https://radimrehurek.com/gensim/models/word2vec.html>\n",
        "\n",
        "\n",
        "* Weng, Lilian (2017). Learning Word Embedding, accessed January 2023, <https://lilianweng.github.io/posts/2017-10-15-word-embedding/word2vec-skip-gram.png>\n",
        "\n",
        "\n",
        "---\n",
        "## Videos and Podcasts\n",
        "\n",
        "* Patel, Dhaval [codebasics] (2021a). Word2Vec Part 2 | Implement word2vec in gensim | | Deep Learning Tutorial 42 with Python. YouTube, accessed December 2022, <https://www.youtube.com/watch?v=Q2NtCcqmIww&t=0s>\n",
        "\n",
        "* Krohn, Jon (2022). SDS 626: Subword Tokenization with Byte-Pair Encoding. Super Data Science Podcast, accessed December 2022, <https://www.superdatascience.com/podcast/subword-tokenization-with-byte-pair-encoding>\n",
        "\n",
        "* Huggingface (2021a). Byte Pair Encoding Tokenization. YouTube, accessed December 2022, <https://www.youtube.com/watch?v=HEikzVL-lZU&t=1s> (and <https://huggingface.co/course/chapter6/5>)\n",
        "\n",
        "* Huggingface (2021b). WordPiece Tokenization. YouTube, accessed December 2022, <https://www.youtube.com/watch?v=qpv6ms_t_1A> (and <https://huggingface.co/course/chapter6/6>)"
      ],
      "metadata": {
        "id": "L9_8zyG_Ih_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acknowledgment\n",
        "\n",
        "Notebook texts were powered by OpenAI's ChatGPT."
      ],
      "metadata": {
        "id": "_NoSrODSIOMB"
      }
    }
  ]
}